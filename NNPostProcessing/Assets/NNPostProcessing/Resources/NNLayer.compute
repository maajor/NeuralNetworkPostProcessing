// neural network post-processing
#pragma kernel LeakyReLU THREADS=1
#pragma kernel BatchNormalization THREADS=1 
#pragma kernel InputLayer THREADS=1
#pragma kernel OutputLayer  THREADS=1
#pragma kernel UpSampling2D THREADS=1
#pragma kernel ReLU THREADS=1
#pragma kernel Tanh THREADS=1 
#pragma kernel Add THREADS=1 
#pragma kernel Concatenate THREADS=1
#pragma kernel Conv2D_8 THREADS=8
#pragma kernel Conv2D_12 THREADS=12
#pragma kernel Conv2D_16 THREADS=16
#pragma kernel Conv2D_20 THREADS=20
#pragma kernel Conv2D_32 THREADS=32
#pragma kernel Conv2D_64 THREADS=64
#pragma kernel Conv2D_128 THREADS=128
#pragma kernel Conv2D_256 THREADS=256

Texture2D<half3> InputImage;
Texture2D<half> InputImage1;
RWTexture2D<half3> OutputImage;

Buffer<half> LayerInput0;
Buffer<half> LayerInput1;
RWBuffer<half> LayerOutput;
Buffer<half> Weights;
uniform uint4 WeightsShape;//for conv2d: n_Hk, n_Wk, n_Ci, n_Ck; for other: size, 0, 0, 0
uniform uint2 Stride;
uniform uint3 InputShape;//n_Hi, n_Wi, n_Ci
uniform uint3 OutputShape;//n_Ho, n_Wo, n_Co=n_Ck
uniform uint3 InputShapeIdMultiplier;
uniform uint3 InputShapeIdMultiplier1;
uniform uint3 OutputShapeIdMultiplier;
uniform uint4 WeightsShapeIdMultiplier;
uniform uint2 Size;
uniform half Alpha;

#define n_Wk WeightsShape.y
#define n_Hk WeightsShape.x
#define n_Ck WeightsShape.w
#define n_Wi InputShape.y
#define n_Hi InputShape.x
#define n_Ci InputShape.z
#define n_Wo OutputShape.y
#define n_Ho OutputShape.x
#define n_Co OutputShape.z

groupshared float cache[THREADS*4];

[numthreads(THREADS, 1, 4)]
#define KERNEL_NAME(x, y) x##_##y
void KERNEL_NAME(Conv2D, THREADS)
(uint3 id : SV_DispatchThreadID, uint3 groupid : SV_GroupThreadID)
{
	//id: outputshape
	uint2 InputId = id.zy * Stride;

	half bias = Weights[n_Wk * n_Hk * n_Ci * n_Ck + id.x];
	half conv = 0;

	uint3 offset = uint3((n_Wk - 1) / 2, (n_Hk - 1) / 2, 0);
	
	//each kernel x
	for (uint p = 0; p < n_Wk; p++) {
		//each kernel y
		for (uint q = 0; q < n_Hk; q++) {
			
			int3 input_id = int3(InputId.xy, id.x) - int3(offset) + int3(p, q, 0);

			if (id.x < n_Ci) {
				//reflect padding
				input_id.x = input_id.x < 0 ? -input_id.x : input_id.x;
				input_id.y = input_id.y < 0 ? -input_id.y : input_id.y;
				input_id.x = input_id.x > (int)InputShape.x ? 2 * (int)InputShape.x - input_id.x : input_id.x;
				input_id.y = input_id.y > (int)InputShape.y ? 2 * (int)InputShape.y - input_id.y : input_id.y;
				
				cache[groupid.z * THREADS + id.x] = LayerInput0[dot(input_id.xyz, InputShapeIdMultiplier)];
			}
			
			GroupMemoryBarrierWithGroupSync();
			
			//each layer input, n_Ci = kernel z
			for (uint w = 0; w < n_Ci; w++) {
				conv += cache[groupid.z * THREADS + w] * Weights[dot(uint4(p, q, w, id.x), WeightsShapeIdMultiplier)];
			}

			GroupMemoryBarrierWithGroupSync();
		}
	}
	if (id.x < n_Co) {
		LayerOutput[dot(id.zyx, OutputShapeIdMultiplier)] = conv + bias;
	}
}

[numthreads(32, 1, 1)]
void ReLU(uint3 id : SV_DispatchThreadID)
{
	LayerOutput[id.x] = LayerInput0[id.x] > 0 ? LayerInput0[id.x] : 0;
}

[numthreads(32, 1, 1)]
void Tanh(uint3 id : SV_DispatchThreadID)
{
	LayerOutput[id.x] = tanh(LayerInput0[id.x]);
}

[numthreads(32, 1, 1)]
void LeakyReLU(uint3 id : SV_DispatchThreadID)
{
	LayerOutput[id.x] = LayerInput0[id.x] > 0 ? LayerInput0[id.x] : LayerInput0[id.x] * Alpha;
}

[numthreads(8, 8, 1)]
void Concatenate(uint3 id : SV_DispatchThreadID)
{
	if (id.z < InputShape.z) {
		LayerOutput[dot(id, OutputShapeIdMultiplier)] = LayerInput0[dot(id, InputShapeIdMultiplier)];
	}
	else {
		LayerOutput[dot(id, OutputShapeIdMultiplier)] = LayerInput1[dot(uint3(id.xy, id.z - InputShape.z), InputShapeIdMultiplier1)];
	}
}

[numthreads(8, 8, 1)]
void Add(uint3 id : SV_DispatchThreadID)
{
	LayerOutput[dot(id, OutputShapeIdMultiplier)] = LayerInput0[dot(id, InputShapeIdMultiplier)] + LayerInput1[dot(id, InputShapeIdMultiplier)];
}

//momentum trained, take parameter as population mean/std
[numthreads(32, 1, 1)]
void BatchNormalization(const uint3 id : SV_DispatchThreadID)
{
	half gamma			= Weights[id.y * 4];
	half beta			= Weights[id.y * 4 + 1];
	half mov_mean		= Weights[id.y * 4 + 2];
	half mov_variance	= Weights[id.y * 4 + 3];

	half rescale = sqrt(mov_variance + 1e-3);
	rescale = gamma / rescale;
	uint idx = id.x * n_Ci + id.y;
	LayerOutput[idx] = LayerInput0[idx] * rescale - mov_mean * rescale + beta;
}

[numthreads(8, 8, 1)]
void UpSampling2D(uint3 id : SV_DispatchThreadID)
{
	half2 inputid = (half2)id.xy / (half2)Size.xy;
	uint3 floor_inputid = uint3(floor(inputid), id.z);
	half2 frac_inputid = inputid - floor_inputid.xy;
	half bilinear_interp =
		LayerInput0[dot(floor_inputid, InputShapeIdMultiplier)] * (1 - frac_inputid.x) * (1 - frac_inputid.y) +
		LayerInput0[dot(floor_inputid + uint3(1, 0, 0), InputShapeIdMultiplier)] * (frac_inputid.x) * (1 - frac_inputid.y) +
		LayerInput0[dot(floor_inputid + uint3(0, 1, 0), InputShapeIdMultiplier)] * (1 - frac_inputid.x) * (frac_inputid.y) +
		LayerInput0[dot(floor_inputid + uint3(1, 1, 0), InputShapeIdMultiplier)] * (frac_inputid.x) * (frac_inputid.y);
	LayerOutput[dot(id, OutputShapeIdMultiplier)] = bilinear_interp;
}

[numthreads(8, 8, 1)]
void InputLayer(uint3 id : SV_DispatchThreadID)
{
	uint2 remapid = uint2(id.y, InputShape.x - 1 - id.x);
	half3 remap = InputImage[remapid.xy] * 2.0f - 1.0f;
	/*if (InputShape.z == 4) {
		half3 remap_dep = InputImage1[remapid.xy] * 2.0f - 1.0f;
		LayerOutput[dot(uint3(id.xy, 3), InputShapeIdMultiplier)] = remap_dep.x;
	}*/
	LayerOutput[dot(uint3(id.xy, 0), InputShapeIdMultiplier)] = remap.x;
	LayerOutput[dot(uint3(id.xy, 1), InputShapeIdMultiplier)] = remap.y;
	LayerOutput[dot(uint3(id.xy, 2), InputShapeIdMultiplier)] = remap.z;
}

[numthreads(8, 8, 1)]
void OutputLayer(uint3 id : SV_DispatchThreadID)
{
	uint2 remapid = uint2(id.y, InputShape.x - 1 - id.x);
	OutputImage[remapid.xy] = saturate(half3(
		LayerInput0[dot(uint3(id.xy, 0), InputShapeIdMultiplier)] * 0.5f + 0.5f,
		LayerInput0[dot(uint3(id.xy, 1), InputShapeIdMultiplier)] * 0.5f + 0.5f,
		LayerInput0[dot(uint3(id.xy, 2), InputShapeIdMultiplier)] * 0.5f + 0.5f));
}